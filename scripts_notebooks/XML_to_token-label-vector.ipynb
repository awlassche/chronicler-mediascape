{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import fasttext\n",
    "import fileinput\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "TOKENIZER = nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser()\n",
    "xml = etree.parse('../chronicles/1789_Brug_Wall/1789_Brug_Wall_tei.xml')\n",
    "\n",
    "for elem in xml.getiterator():\n",
    "    elem.tag = etree.QName(elem).localname\n",
    "etree.cleanup_namespaces(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"[{`,.?!:;/\\()''\"\"¬}]\" #eventueel hier de hyphen weghalen zodat deze later gemerged kunnen worden in het outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e501f210f5744dcf8214c1dab766b5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1291), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "previous_row = dict()\n",
    "tokens = []\n",
    "\n",
    "line_elements = xml.xpath('//l')\n",
    "for i, line in tqdm(enumerate(line_elements),\n",
    "                   total=len(line_elements)):\n",
    "    for element in line.xpath('child::text()|*'):\n",
    "        if type(element) == etree._ElementUnicodeResult:\n",
    "            label = 'O'\n",
    "            attribute = ''\n",
    "            wordstring = re.sub(r\"((¬#?) ?)\", \"\", str(element)).lower()\n",
    "            for c in wordstring:\n",
    "                if c in punctuation:\n",
    "                    wordstring = wordstring.replace(c, '')\n",
    "            for token in TOKENIZER(str(wordstring)):\n",
    "                tokens.append(dict(sentence_id = i,\n",
    "                               token = token,\n",
    "                               label = label,\n",
    "                               attribute = attribute\n",
    "                               ))\n",
    "                previous_row = dict()\n",
    "        else:\n",
    "            if len(previous_row) == 0:\n",
    "                label = element.xpath('name()') + '-B'\n",
    "            else:\n",
    "                if previous_row['label'] == (element.xpath('name()') + '-I') or previous_row['label'] == (element.xpath('name()') + '-B'):\n",
    "                    label = element.xpath('name()') + '-I'\n",
    "                else:\n",
    "                    label = element.xpath('name()') + '-B'\n",
    "            text = ''.join(element.xpath('descendant::text()'))\n",
    "            if label == 'hi':\n",
    "                label = ''\n",
    "                attribute = ''\n",
    "            if label == 'waarneming-B' or label == 'waarneming-I':\n",
    "                attribute = ''.join(element.xpath('@waarneming'))\n",
    "            wordstring = str(text).lower()\n",
    "            for c in wordstring:\n",
    "                if c in punctuation:\n",
    "                    wordstring = wordstring.replace(c, '')\n",
    "            for j, token in (enumerate(TOKENIZER(str(wordstring)))):\n",
    "                if j > 0 and label != '':\n",
    "                    label = element.xpath('name()') + '-I'    \n",
    "                tokens.append(dict(sentence_id = i,\n",
    "                                   token = token,\n",
    "                                   label = label,\n",
    "                                   attribute = attribute\n",
    "                                   ))\n",
    "                previous_row = dict(sentence_id = i,\n",
    "                                   token = token,\n",
    "                                   label = label,\n",
    "                                   attribute = attribute\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tokenized_text = pd.DataFrame(tokens)\n",
    "\n",
    "tokenized_text.to_csv('../output/1789_Brug_Wall/tokenized_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'informatiebron-B', 'waarneming-B', 'ontvanger-B',\n",
       "       'informatiebron-I', 'waarneming-I', 'ontvanger-I'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = xml.find('//title')\n",
    "fname = title.text\n",
    "text = xml.find('//text')\n",
    "chronicle = ''.join(text.itertext())\n",
    "wordstring = re.sub(r\"((¬#?) ?)\", \"\", chronicle.lower())\n",
    "for c in wordstring:\n",
    "    if c in punctuation:\n",
    "        wordstring = wordstring.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(fname) + '.txt', 'w') as f:\n",
    "    f.write(str(TOKENIZER(wordstring)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge multiple files to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"/Users/alielassche/documents/github/chronicling-sources/fasttext/xxxx_Brug_Wall/*.txt\")\n",
    "\n",
    "with open('xxxx_Brug_Wall.txt', 'w') as file:\n",
    "    input_lines = fileinput.input(file_list)\n",
    "    file.writelines(input_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"../fasttext/xxxx_Brug_Wall/xxxx_Brug_Wall_kopie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"../fasttext/xxxx_Brug_Wall/xxxx_Brug_Wall.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"../fasttext/alle_Brug_Wall/alle_Brug_Wall.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding vectors to feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = pd.read_csv('../output/1789_Brug_Wall/tokenized_text.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93299691784b0cbedf0b29cc1c8aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for index, row in tqdm(tokenized_text.iterrows()):\n",
    "    row_dict = dict(row)\n",
    "    row_dict['vector'] = model.get_word_vector(row_dict['token'])\n",
    "    all_rows.append(row_dict)\n",
    "\n",
    "feature_file = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file.to_csv('../output/1789_Brug_Wall/feature_file.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_files = glob.glob(os.path.join('../output/alle_Brug_Wall/*.csv'))\n",
    "\n",
    "file_list = []\n",
    "for file in all_feature_files:\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    file_list.append(df)\n",
    "\n",
    "total = pd.concat(file_list, ignore_index=True, sort=False).set_index('token').drop(['Unnamed: 0', 'sentence_id'], 1)\n",
    "total.to_csv('../output/alle_Brug_Wall/alle_Brug_Wall_concat.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
