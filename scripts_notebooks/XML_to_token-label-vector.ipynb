{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import fasttext\n",
    "import fileinput\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "TOKENIZER = nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser()\n",
    "xml = etree.parse('FILEPATH_TO_XML')\n",
    "\n",
    "for elem in xml.getiterator():\n",
    "    elem.tag = etree.QName(elem).localname\n",
    "etree.cleanup_namespaces(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"[{`,.?!:;/\\()''\"\"¬}]\" #eventueel hier de hyphen weghalen zodat deze later gemerged kunnen worden in het outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_row = dict()\n",
    "tokens = []\n",
    "\n",
    "line_elements = xml.xpath('//l')\n",
    "for i, line in tqdm(enumerate(line_elements),\n",
    "                   total=len(line_elements)):\n",
    "    for element in line.xpath('child::text()|*'):\n",
    "        if type(element) == etree._ElementUnicodeResult:\n",
    "            label = 'O'\n",
    "            attribute = ''\n",
    "            wordstring = re.sub(r\"((¬#?) ?)\", \"\", str(element)).lower()\n",
    "            for c in wordstring:\n",
    "                if c in punctuation:\n",
    "                    wordstring = wordstring.replace(c, '')\n",
    "            for token in TOKENIZER(str(wordstring)):\n",
    "                tokens.append(dict(sentence_id = i,\n",
    "                               token = token,\n",
    "                               label = label,\n",
    "                               attribute = attribute\n",
    "                               ))\n",
    "                previous_row = dict()\n",
    "        else:\n",
    "            if len(previous_row) == 0:\n",
    "                label = element.xpath('name()') + '-B'\n",
    "            else:\n",
    "                if previous_row['label'] == (element.xpath('name()') + '-I') or previous_row['label'] == (element.xpath('name()') + '-B'):\n",
    "                    label = element.xpath('name()') + '-I'\n",
    "                else:\n",
    "                    label = element.xpath('name()') + '-B'\n",
    "            text = ''.join(element.xpath('descendant::text()'))\n",
    "            if label == 'hi':\n",
    "                label = ''\n",
    "                attribute = ''\n",
    "            if label == 'waarneming-B' or label == 'waarneming-I':\n",
    "                attribute = ''.join(element.xpath('@waarneming'))\n",
    "            wordstring = str(text).lower()\n",
    "            for c in wordstring:\n",
    "                if c in punctuation:\n",
    "                    wordstring = wordstring.replace(c, '')\n",
    "            for j, token in (enumerate(TOKENIZER(str(wordstring)))):\n",
    "                if j > 0 and label != '':\n",
    "                    label = element.xpath('name()') + '-I'    \n",
    "                tokens.append(dict(sentence_id = i,\n",
    "                                   token = token,\n",
    "                                   label = label,\n",
    "                                   attribute = attribute\n",
    "                                   ))\n",
    "                previous_row = dict(sentence_id = i,\n",
    "                                   token = token,\n",
    "                                   label = label,\n",
    "                                   attribute = attribute\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tokenized_text = pd.DataFrame(tokens)\n",
    "\n",
    "tokenized_text.to_csv('TOKENIZED_TEXT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = xml.find('//title')\n",
    "fname = title.text\n",
    "text = xml.find('//text')\n",
    "chronicle = ''.join(text.itertext())\n",
    "wordstring = re.sub(r\"((¬#?) ?)\", \"\", chronicle.lower())\n",
    "for c in wordstring:\n",
    "    if c in punctuation:\n",
    "        wordstring = wordstring.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(fname) + '.txt', 'w') as f:\n",
    "    f.write(str(TOKENIZER(wordstring)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge multiple files to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"FILEPATH_TO_TXT-FILES/*.txt\")\n",
    "\n",
    "with open('MERGED_TXT_FILE', 'w') as file:\n",
    "    input_lines = fileinput.input(file_list)\n",
    "    file.writelines(input_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"MERGED_TXT_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"MODEL.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"MODEL.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding vectors to token-label-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = pd.read_csv('TOKENIZED_TEXT.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "for index, row in tqdm(tokenized_text.iterrows()):\n",
    "    row_dict = dict(row)\n",
    "    row_dict['vector'] = model.get_word_vector(row_dict['token'])\n",
    "    all_rows.append(row_dict)\n",
    "\n",
    "feature_file = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file.to_csv('TOKEN-LABEL-VECTOR-FILE.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge token-label-vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_files = glob.glob(os.path.join('FILEPATH_TO_TOKEN-LABEL-VECTOR-FILES/*.csv'))\n",
    "\n",
    "file_list = []\n",
    "for file in all_feature_files:\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    file_list.append(df)\n",
    "\n",
    "total = pd.concat(file_list, ignore_index=True, sort=False).set_index('token').drop(['Unnamed: 0', 'sentence_id'], 1)\n",
    "total.to_csv('MERGED_TOKEN-LABEL-VECTOR-FILE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
